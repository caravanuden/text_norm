{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from nltk import FreqDist\n",
    "import time\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, Embedding, Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.recurrent import LSTM\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size = 250\n",
    "target_vocab_size = 346    # 1000 in full dataset\n",
    "# target_vocab_size = 1000\n",
    "num_samples = 200000\n",
    "context_size = 3\n",
    "padding_entity = [0]\n",
    "self_sil_retention_percent = 0.5\n",
    "X_seq_len = 60\n",
    "y_seq_len = 20\n",
    "hidden = 256\n",
    "layers = 2\n",
    "epochs = 5    # used just 1 for full dataset\n",
    "batch_size = 128\n",
    "val_split = 0.1\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer created\n",
      "Encoder layer created\n",
      "Decoder layer created\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 60, 256)           64512     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 60, 512)           1050624   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 60, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 20, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 20, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 347)           89179     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 347)           0         \n",
      "=================================================================\n",
      "Total params: 5,666,907\n",
      "Trainable params: 5,666,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compiling model before loading any data (some GPUs fail to compile if data sets are large)\n",
    "model = Sequential()\n",
    "\n",
    "# Creating encoder network\n",
    "model.add(Embedding(input_vocab_size+2, hidden, input_length=X_seq_len, mask_zero=True))\n",
    "print('Embedding layer created')\n",
    "model.add(Bidirectional(LSTM(hidden, return_sequences = True), merge_mode = 'concat'))\n",
    "model.add(Bidirectional(LSTM(hidden, return_sequences = True), merge_mode = 'concat'))\n",
    "model.add(Bidirectional(LSTM(hidden), merge_mode = 'concat'))\n",
    "model.add(RepeatVector(y_seq_len))\n",
    "print('Encoder layer created')\n",
    "\n",
    "# Creating decoder network\n",
    "for _ in range(layers):\n",
    "    model.add(LSTM(hidden, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(target_vocab_size+1)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print('Decoder layer created')\n",
    "\n",
    "# checking the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded in 10.44523811340332 s.\n",
      "(9918441, 5)\n",
      "(200000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Brillantaisia</td>\n",
       "      <td>Brillantaisia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>genus</td>\n",
       "      <td>genus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id  class         before          after\n",
       "0            0         0  PLAIN  Brillantaisia  Brillantaisia\n",
       "1            0         1  PLAIN             is             is\n",
       "2            0         2  PLAIN              a              a\n",
       "3            0         3  PLAIN          genus          genus\n",
       "4            0         4  PLAIN             of             of"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Load training data\n",
    "X_train_data = pd.read_csv(\"en_train.csv\")\n",
    "X_train_data['before'] = X_train_data['before'].apply(str)\n",
    "X_train_data['after'] = X_train_data['after'].apply(str)\n",
    "\n",
    "# Class counts\n",
    "# DATE = 258,348\n",
    "# LETTERS = 152,795\n",
    "# CARDINAL = 133744\n",
    "# VERBATIM - has lots of special symbols - 78108\n",
    "# MEASURE = 14783\n",
    "# MONEY = 6128\n",
    "\n",
    "print('Training data loaded in {0} s.'.format(time.time()-start))\n",
    "print(X_train_data.shape)\n",
    "X_train_data = X_train_data.iloc[:num_samples]\n",
    "print(X_train_data.shape)\n",
    "X_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab created in 3.219439744949341 s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Create vocabularies\n",
    "# Target vocab\n",
    "y = list(np.where(X_train_data['class'] == \"PUNCT\", \"sil.\",\n",
    "      np.where(X_train_data['before'] == X_train_data['after'], \"<self>\",\n",
    "               X_train_data['after'])))\n",
    "\n",
    "y = [token.split() for token in y]\n",
    "dist = FreqDist(np.hstack(y))\n",
    "temp = dist.most_common(target_vocab_size-1)\n",
    "temp = [word[0] for word in temp]\n",
    "temp.insert(0, 'ZERO')\n",
    "temp.append('UNK')\n",
    "\n",
    "target_vocab = {word:ix for ix, word in enumerate(temp)}\n",
    "target_vocab_reversed = {ix:word for word,ix in target_vocab.items()}\n",
    "\n",
    "# Input vocab\n",
    "X = list(X_train_data['before'])\n",
    "X = [list(token) for token in X]\n",
    "\n",
    "dist = FreqDist(np.hstack(X))\n",
    "temp = dist.most_common(input_vocab_size-1)\n",
    "temp = [char[0] for char in temp]\n",
    "temp.insert(0, 'ZERO')\n",
    "temp.append('<norm>')\n",
    "temp.append('UNK')\n",
    "\n",
    "input_vocab = {char:ix for ix, char in enumerate(temp)}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('Vocab created in {0} s.'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced tokens with integers in 0.4380378723144531 s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Converting input and target tokens to index values\n",
    "X = index(X, input_vocab)\n",
    "y = index(y, target_vocab)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('Replaced tokens with integers in {0} s.'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added context window to X in 0.8442680835723877 s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Adding a context window of 3 words in Input, with token separated by <norm>\n",
    "X = add_context_window(X, context_size, padding_entity, input_vocab)\n",
    "\n",
    "print('Added context window to X in {0} s.'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added padding and converted to np array in 3.2257697582244873 s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "X = batch_wise_padding(X, X_seq_len) # Padding\n",
    "y = batch_wise_padding(y, y_seq_len)\n",
    "\n",
    "# Convert X_test to integer array, batch-wise (converting full data to array at once takes a lot of time)\n",
    "X = array_batchwise(X, X_seq_len)\n",
    "y = array_batchwise(y, y_seq_len)\n",
    "\n",
    "print('Added padding and converted to np array in {0} s.'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 4485s 22ms/step - loss: 0.2073 - acc: 0.9525\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 4364s 22ms/step - loss: 0.1219 - acc: 0.9741\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 4362s 22ms/step - loss: 0.0564 - acc: 0.9874\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 4360s 22ms/step - loss: 0.0397 - acc: 0.9911\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 4371s 22ms/step - loss: 0.0367 - acc: 0.9911\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "y_sequences = sequences(y, y_seq_len, target_vocab)\n",
    "\n",
    "print('Fitting model...')\n",
    "\n",
    "#Fitting the model on the validation data with batch size set to 128 for a total of 10 epochs:\n",
    "history_full_data = model.fit(np.asarray(X), np.asarray(y_sequences), batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('text_norm_model_full.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights from training\n",
    "model.load_weights('text_norm_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Another</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>religious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id     before\n",
       "0            0         0    Another\n",
       "1            0         1  religious\n",
       "2            0         2     family\n",
       "3            0         3         is\n",
       "4            0         4         of"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare test data in the right format\n",
    "X_test_data = pd.read_csv(\"en_test.csv\")\n",
    "X_test_data['before'] = X_test_data['before'].apply(str)\n",
    "X_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = X_test_data.iloc[:10000]\n",
    "X_test = list(X_test_data['before'])\n",
    "X_test = [list(token) for token in X_test]\n",
    "\n",
    "X_test = index(X_test, input_vocab) # Convert to integer index\n",
    "X_test = add_context_window(X_test, context_size, padding_entity, input_vocab) # Add context window\n",
    "X_test = batch_wise_padding(X_test, X_seq_len) # Padding\n",
    "\n",
    "# Convert X_test to integer array, batch-wise (converting full data to array at once takes a lot of time)\n",
    "X_test = array_batchwise(X_test, X_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions done for 0/10000 samples \n",
      "10000/10000 [==============================] - 78s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "# Predicting for 1000 sequences at a time\n",
    "for i in range(0, len(X_test), 10000):\n",
    "    if i + 10000 >= len(X_test):\n",
    "        i_end = len(X_test)\n",
    "    else:\n",
    "        i_end = i + 10000\n",
    "    X_test_small = X_test[i:i_end]\n",
    "    print('Predictions done for {}/{} samples '.format(i, len(X_test)))\n",
    "    test_predictions = np.argmax(model.predict(np.asarray(X_test_small), batch_size = 64, verbose=1), axis=2)\n",
    "\n",
    "predicted_test_sequences = []\n",
    "for prediction in test_predictions:\n",
    "    sequence = ' '.join([target_vocab_reversed[index] for index in prediction if index > 0])\n",
    "    predicted_test_sequences.append(sequence)\n",
    "np.savetxt('test_result', predicted_test_sequences, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
